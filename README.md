TWLDA (Term Weighting LDA)
===

TWLDA is a new topic of LDA which assigns low weights to words with low topic discriminating power. For more details, please refer to "Exploring Topic Discriminating Power of Words in Latent Dirichlet
Allocation" created by Kai Yang, Yi Cai and Zhenhong Chen.

## MCMC sample algorithm

<a herf="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a>

## Metropolis-Hastings algorithm

<a herf="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolisâ€“Hastings algorithm</a>

## Unigram Model
$\vec{p} = (p_1, p_2, ..., p_v)$ refers to the probability of each word to be chosed.
So the result is $w \sim Mult(w|\vec{p})$
For a doc created using Unigram Model, the probability that the doc to be created is
$$p(\vec{w}) = p(w_1, w_2, ..., w_n) = p(w_1)p(w_2)...p(w_n)$$
And for the hypothesis that each doc is independent, a corpus with some doc $W = (\vec{w_1}),(\vec{w_2}),...,(\vec{w_m})$, the probability is
$$p(W) = p(\vec{w_1})p(\vec{w_2})...p(\vec{w_m})$$
Assume there are N words in the corpus, for each word $v_i$ has appeard $n_i$ times, $\vec{n}=(n_1,n_2,...,n_v)$ will be a Multinomial Distribution
$$p(\vec{n})=Mult(\vec{n}|\vec{p},N)=\binom{N}{\vec{n}}\prod_{k=1}^Vp_k^{n_k}$$
Now the propability of corpus is
$$p(W)=p(\vec{w_1}\vec{w_2}...\vec{w_m})=\prod_{k=1}^Vp_k^{n_k}$$
So $$\hat{p_i}=\frac{n_i}{N}$$

## Bayes Unigram Model
$$\hat{p_i}=\frac{n_i+\alpha_i}{\Sigma_{i=1}^V(n_i+\alpha_i)}$$

## LDA Model 
### Two most important  formula
$$\vec{\alpha}\to\vec{\theta}_m\to z_{m,n}$$
$$\vec{\beta}\to\vec{\varphi}_k\to w_{m,n}|k=z_{m,n}$$

### Gibbs Sampling 
$$p(z_i=k|\vec{z}_{\lnot i},\vec{w}) \varpropto \frac{n_{m,\lnot i}^{(k)}+\alpha_k}{\Sigma_{k=1}^K(n_{m,\lnot i}^{(t)}+\alpha_k)}\cdot\frac{n_{k,\lnot i}^{(t)}+\beta_t}{\Sigma_{t=1}^V(n_{k,\lnot i}^{(t)}+\beta_t)}$$

## TWLDA
### Step1: $\vec{\varphi'}\gets TopicModel()$
In Step 1, a topic model is executed. Then a topic-word distribution $\vec{\varphi'}$ is generated by this topic model.

### Step2: $\vec{\sigma}\gets Calculate(\vec{\varphi'})$
In Step2, according to the $\vec{\varphi'}$, we apply a supervised term weighting scheme to calculate weights of word $\vec{\sigma}$. Since supervised schemes have the ability to measure the topic discriminating power of words, in principle, all the supervised term weighting schemes can be applied here.

### Step3: Discounting the number of words
Step3 is to discount the number of words by their weights. The number of words is diminished proportionally accoding to weights of words. Hence, the total discounted number of words in document $m$ under topic $k$ is calculated as follow:
$$n'{_m^{(k)}}=\sum_{t=1}^V\sigma_tn_{mkt}$$
where $\sigma_t$ denates the weight of word $t$, which is ranging from 0 to 1. $n_{nkt}$ is the number of word $t$ belonging to topic $k$ in document $m$. Similarly, the total discounted number of word $t$ under topic $k$ is calculated as follows:
$$n'{_k^{(t)}}=\sum_{m=1}^M\sigma_tn_{mkt}$$

### Step4: Executing xLDA with the discounted values
Step4 is to excute the standard LDA or its variants, denoted as xLDA, using the discounted calues calculated in Equations 1 and 2.