TWLDA (Term Weighting LDA)
===

TWLDA is a new topic of LDA which assigns low weights to words with low topic discriminating power. For more details, please refer to "Exploring Topic Discriminating Power of Words in Latent Dirichlet Allocation" created by Kai Yang, Yi Cai and Zhenhong Chen.

## MCMC sample algorithm

<a herf="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a>

## Metropolis-Hastings algorithm

<a herf="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolisâ€“Hastings algorithm</a>

## Unigram Model
<img src="https://latex.codecogs.com/png.latex?\vec{p}&space;=&space;(p_1,&space;p_2,&space;...,&space;p_v)" title="\vec{p} = (p_1, p_2, ..., p_v)" /> refers to the probability of each word to be chosed.
So the result is <img src="https://latex.codecogs.com/png.latex?w&space;\sim&space;Mult(w|\vec{p})" title="w \sim Mult(w|\vec{p})" />
For a doc created using Unigram Model, the probability that the doc to be created is

<img src="https://latex.codecogs.com/png.latex?p(\vec{w})&space;=&space;p(w_1,&space;w_2,&space;...,&space;w_n)&space;=&space;p(w_1)p(w_2)...p(w_n)" title="p(\vec{w}) = p(w_1, w_2, ..., w_n) = p(w_1)p(w_2)...p(w_n)" />

And for the hypothesis that each doc is independent, a corpus with some doc <img src="https://latex.codecogs.com/png.latex?W&space;=&space;(\vec{w_1}),(\vec{w_2}),...,(\vec{w_m})" title="W = (\vec{w_1}),(\vec{w_2}),...,(\vec{w_m})" />, the probability is

<img src="https://latex.codecogs.com/png.latex?p(W)&space;=&space;p(\vec{w_1})p(\vec{w_2})...p(\vec{w_m})" title="p(W) = p(\vec{w_1})p(\vec{w_2})...p(\vec{w_m})" />

Assume there are N words in the corpus, for each word <img src="https://latex.codecogs.com/png.latex?v_i" title="v_i" /> has appeard <img src="https://latex.codecogs.com/png.latex?n_i" title="n_i" /> times, <img src="https://latex.codecogs.com/png.latex?\vec{n}=(n_1,n_2,...,n_v)" title="\vec{n}=(n_1,n_2,...,n_v)" /> will be a Multinomial Distribution

<img src="https://latex.codecogs.com/png.latex?p(\vec{n})=Mult(\vec{n}|\vec{p},N)=\binom{N}{\vec{n}}\prod_{k=1}^Vp_k^{n_k}" title="p(\vec{n})=Mult(\vec{n}|\vec{p},N)=\binom{N}{\vec{n}}\prod_{k=1}^Vp_k^{n_k}" />

Now the propability of corpus is

<img src="https://latex.codecogs.com/png.latex?p(W)=p(\vec{w_1}\vec{w_2}...\vec{w_m})=\prod_{k=1}^Vp_k^{n_k}" title="p(W)=p(\vec{w_1}\vec{w_2}...\vec{w_m})=\prod_{k=1}^Vp_k^{n_k}" />

So 

<img src="https://latex.codecogs.com/png.latex?\hat{p_i}=\frac{n_i}{N}" title="\hat{p_i}=\frac{n_i}{N}" />

## Bayes Unigram Model

<img src="https://latex.codecogs.com/png.latex?\hat{p_i}=\frac{n_i&plus;\alpha_i}{\Sigma_{i=1}^V(n_i&plus;\alpha_i)}" title="\hat{p_i}=\frac{n_i+\alpha_i}{\Sigma_{i=1}^V(n_i+\alpha_i)}" />

## LDA Model 
### Two most important  formula

<img src="https://latex.codecogs.com/png.latex?\vec{\alpha}\to\vec{\theta}_m\to&space;z_{m,n}" title="\vec{\alpha}\to\vec{\theta}_m\to z_{m,n}" />

<img src="https://latex.codecogs.com/png.latex?\vec{\beta}\to\vec{\varphi}_k\to&space;w_{m,n}|k=z_{m,n}" title="\vec{\beta}\to\vec{\varphi}_k\to w_{m,n}|k=z_{m,n}" />

### Gibbs Sampling 

<img src="https://latex.codecogs.com/png.latex?p(z_i=k|\vec{z}_{\lnot&space;i},\vec{w})&space;\varpropto&space;\frac{n_{m,\lnot&space;i}^{(k)}&plus;\alpha_k}{\Sigma_{k=1}^K(n_{m,\lnot&space;i}^{(t)}&plus;\alpha_k)}\cdot\frac{n_{k,\lnot&space;i}^{(t)}&plus;\beta_t}{\Sigma_{t=1}^V(n_{k,\lnot&space;i}^{(t)}&plus;\beta_t)}" title="p(z_i=k|\vec{z}_{\lnot i},\vec{w}) \varpropto \frac{n_{m,\lnot i}^{(k)}+\alpha_k}{\Sigma_{k=1}^K(n_{m,\lnot i}^{(t)}+\alpha_k)}\cdot\frac{n_{k,\lnot i}^{(t)}+\beta_t}{\Sigma_{t=1}^V(n_{k,\lnot i}^{(t)}+\beta_t)}" />

## TWLDA
### Step1: <img src="https://latex.codecogs.com/png.latex?\vec{\varphi'}\gets&space;TopicModel()" title="\vec{\varphi'}\gets TopicModel()" />
In Step 1, a topic model is executed. Then a topic-word distribution <img src="https://latex.codecogs.com/png.latex?\vec{\varphi'}" title="\vec{\varphi'}" /> is generated by this topic model.

### Step2: <img src="https://latex.codecogs.com/png.latex?\vec{\sigma}\gets&space;Calculate(\vec{\varphi'})" title="\vec{\sigma}\gets Calculate(\vec{\varphi'})" />
In Step2, according to the <img src="https://latex.codecogs.com/png.latex?\vec{\varphi'}" title="\vec{\varphi'}" />, we apply a supervised term weighting scheme to calculate weights of word <img src="https://latex.codecogs.com/png.latex?\vec{\sigma}" title="\vec{\sigma}" />. Since supervised schemes have the ability to measure the topic discriminating power of words, in principle, all the supervised term weighting schemes can be applied here.

### Step3: Discounting the number of words
Step3 is to discount the number of words by their weights. The number of words is diminished proportionally accoding to weights of words. Hence, the total discounted number of words in document <img src="https://latex.codecogs.com/png.latex?m" title="m" /> under topic <img src="https://latex.codecogs.com/png.latex?k" title="k" /> is calculated as follow:

<img src="https://latex.codecogs.com/png.latex?n'{_m^{(k)}}=\sum_{t=1}^V\sigma_tn_{mkt}" title="n'{_m^{(k)}}=\sum_{t=1}^V\sigma_tn_{mkt}" />

where <img src="https://latex.codecogs.com/png.latex?\sigma_t" title="\sigma_t" /> denates the weight of word <img src="https://latex.codecogs.com/png.latex?t" title="t" />, which is ranging from 0 to 1. <img src="https://latex.codecogs.com/png.latex?n_{nkt}" title="n_{nkt}" /> is the number of word <img src="https://latex.codecogs.com/png.latex?t" title="t" /> belonging to topic <img src="https://latex.codecogs.com/png.latex?k" title="k" /> in document <img src="https://latex.codecogs.com/png.latex?m" title="m" />. Similarly, the total discounted number of word <img src="https://latex.codecogs.com/png.latex?t" title="t" /> under topic <img src="https://latex.codecogs.com/png.latex?k" title="k" /> is calculated as follows:

<img src="https://latex.codecogs.com/png.latex?n'{_k^{(t)}}=\sum_{m=1}^M\sigma_tn_{mkt}" title="n'{_k^{(t)}}=\sum_{m=1}^M\sigma_tn_{mkt}" />

### Step4: Executing xLDA with the discounted values
Step4 is to excute the standard LDA or its variants, denoted as xLDA, using the discounted calues calculated in Equations 1 and 2.
